{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modélisation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import des modules "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Selection\n",
    "from sklearn.model_selection import (\n",
    "    cross_validate,\n",
    ")\n",
    "\n",
    "# Preprocess\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Modèles\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "\n",
    "import sys\n",
    "import importlib\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "import config\n",
    "importlib.reload(config)  # Ensure we get the latest TARGET value\n",
    "from config import TARGET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1539, 26)"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "building_consumption = pd.read_csv(\"../assets/building_consumption_cleaned.csv\")\n",
    "building_consumption.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparaison de différents modèles supervisés"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A réaliser :\n",
    "* Pour chaque algorithme que vous allez tester, vous devez :\n",
    "    * Réaliser au préalable une séparation en jeu d'apprentissage et jeu de test via une validation croisée.\n",
    "    * Si les features quantitatives que vous souhaitez utiliser ont des ordres de grandeur très différents les uns des autres, et que vous utilisez un algorithme de regression qui est sensible à cette différence, alors il faut réaliser un scaling (normalisation) de la donnée au préalable.\n",
    "    * Entrainer le modèle sur le jeu de Train\n",
    "    * Prédire la cible sur la donnée de test (nous appelons cette étape, l'inférence).\n",
    "    * Calculer les métriques de performance R2, MAE et RMSE sur le jeu de train et de test.\n",
    "    * Interpréter les résultats pour juger de la fiabilité de l'algorithme.\n",
    "* Vous pouvez choisir par exemple de tester un modèle linéaire, un modèle à base d'arbres et un modèle de type SVM\n",
    "* Déterminer le modèle le plus performant parmi ceux testés."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modèle linéaire - LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numerical features: 25\n",
      "Categorical features: 0\n",
      "Total features: 25\n",
      "Total columns (excluding target): 25\n",
      "Match: True\n",
      "\n",
      "Numerical features: ['NumberofBuildings', 'PropertyGFAParking', 'PropertyGFABuilding(s)', 'ENERGYSTARScore', 'NumberOfUseTypes', 'SteamUse', 'NaturalGas', 'Electricity', 'PrimaryPropertyType_0', 'PrimaryPropertyType_1', 'PrimaryPropertyType_2', 'PrimaryPropertyType_3', 'PrimaryPropertyType_4', 'BuildingType_Nonresidential COS', 'BuildingType_Nonresidential WA', 'BuildingType_SPS-District K-12', 'NumberofFloors_quintile_Q2', 'NumberofFloors_quintile_Q3', 'NumberofFloors_quintile_Q4', 'NumberofFloors_quintile_Q5', 'EraBuilt_1951-1970', 'EraBuilt_1971-1990', 'EraBuilt_1991-2010', 'EraBuilt_Post-2010', 'EraBuilt_≤1930']\n",
      "\n",
      "Categorical features: []\n"
     ]
    }
   ],
   "source": [
    "# Select features (excluding the target column)\n",
    "features_df = building_consumption.drop(columns=[TARGET])\n",
    "\n",
    "numerical_features = features_df.select_dtypes(include=\"number\").columns.tolist()\n",
    "categorical_features = features_df.select_dtypes(exclude=\"number\").columns.tolist()\n",
    "\n",
    "print(\"Numerical features:\", len(numerical_features))\n",
    "print(\"Categorical features:\", len(categorical_features))\n",
    "print(\"Total features:\", len(numerical_features) + len(categorical_features))\n",
    "print(\"Total columns (excluding target):\", len(features_df.columns))\n",
    "print(\"Match:\", len(features_df.columns) == len(numerical_features) + len(categorical_features))\n",
    "\n",
    "print(\"\\nNumerical features:\", numerical_features)\n",
    "print(\"\\nCategorical features:\", categorical_features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note importante:** Pour une comparaison équitable, tous les modèles utilisent la même stratégie de validation croisée avec `KFold(n_splits=5, shuffle=True, random_state=42)`. Cela garantit que chaque modèle est évalué sur exactement les mêmes divisions train/test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration commune pour l'évaluation des modèles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration commune pour tous les modèles\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Stratégie de validation croisée commune pour tous les modèles\n",
    "cv_strategy = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Métriques d'évaluation communes\n",
    "scoring = [\"r2\", \"neg_mean_squared_error\", \"neg_mean_absolute_error\"]\n",
    "\n",
    "# Données communes\n",
    "X = building_consumption.drop(columns=[TARGET])\n",
    "y = building_consumption[TARGET]\n",
    "\n",
    "def evaluate_model(pipeline, model_name):\n",
    "    \"\"\"\n",
    "    Évalue un modèle avec la stratégie de validation croisée commune\n",
    "    \n",
    "    Args:\n",
    "        pipeline: Le pipeline sklearn à évaluer\n",
    "        model_name: Nom du modèle pour l'affichage\n",
    "        \n",
    "    Returns:\n",
    "        dict: Résultats de la validation croisée\n",
    "    \"\"\"\n",
    "    cv_results = cross_validate(\n",
    "        pipeline,\n",
    "        X,\n",
    "        y,\n",
    "        cv=cv_strategy,\n",
    "        scoring=scoring,\n",
    "        return_train_score=True,\n",
    "    )\n",
    "    \n",
    "    print(f\"=== {model_name} Results ===\")\n",
    "    print(f\"{'Metric':<12} {'Train Mean':<12} {'Train Std':<12} {'Test Mean':<12} {'Test Std':<12}\")\n",
    "    print(\"-\" * 65)\n",
    "    \n",
    "    # Affichage des résultats moyens avec format tabulaire\n",
    "    for metric in scoring:\n",
    "        train_scores = cv_results[f\"train_{metric}\"]\n",
    "        test_scores = cv_results[f\"test_{metric}\"]\n",
    "        # Inverser les scores négatifs pour MAE et RMSE\n",
    "        if \"neg\" in metric:\n",
    "            train_scores = -train_scores\n",
    "            test_scores = -test_scores\n",
    "            metric_name = metric.replace(\"neg_\", \"\").replace(\"_\", \" \").upper()\n",
    "        else:\n",
    "            metric_name = metric.upper()\n",
    "        \n",
    "        print(f\"{metric_name:<12} {train_scores.mean():<12.3f} {train_scores.std():<12.3f} {test_scores.mean():<12.3f} {test_scores.std():<12.3f}\")\n",
    "    \n",
    "    # Calculate RMSE from MSE\n",
    "    rmse_train = np.sqrt(-cv_results[\"train_neg_mean_squared_error\"])\n",
    "    rmse_test = np.sqrt(-cv_results[\"test_neg_mean_squared_error\"])\n",
    "    print(f\"{'RMSE':<12} {rmse_train.mean():<12.3f} {rmse_train.std():<12.3f} {rmse_test.mean():<12.3f} {rmse_test.std():<12.3f}\")\n",
    "    print()  # Ligne vide pour séparation\n",
    "    \n",
    "    return cv_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== LinearRegression Results ===\n",
      "Metric       Train Mean   Train Std    Test Mean    Test Std    \n",
      "-----------------------------------------------------------------\n",
      "R2           0.451        0.025        0.363        0.105       \n",
      "MEAN SQUARED ERROR 178631.446   12086.509    195869.913   52794.949   \n",
      "MEAN ABSOLUTE ERROR 176.095      6.083        181.801      14.133      \n",
      "RMSE         422.407      14.276       438.370      60.842      \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Since all features are numerical (categorical ones were already encoded) \n",
    "# and missing values have been cleaned in the data preparation notebook,\n",
    "# we only need to scale the numerical features\n",
    "if len(categorical_features) > 0:\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"num\", StandardScaler(), numerical_features),\n",
    "            (\"cat\", \"passthrough\", categorical_features),\n",
    "        ]\n",
    "    )\n",
    "else:\n",
    "    # All features are numerical and clean, just apply scaling\n",
    "    preprocessor = StandardScaler()\n",
    "\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', LinearRegression())\n",
    "])\n",
    "\n",
    "# Évaluation avec la fonction commune\n",
    "cv_results = evaluate_model(pipeline, \"LinearRegression\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modèle SVR (Support Vector Regressor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== SVR Results ===\n",
      "Metric       Train Mean   Train Std    Test Mean    Test Std    \n",
      "-----------------------------------------------------------------\n",
      "R2           -0.022       0.002        -0.024       0.009       \n",
      "MEAN SQUARED ERROR 333567.842   33884.583    333744.802   136324.466  \n",
      "MEAN ABSOLUTE ERROR 134.183      2.412        134.902      10.082      \n",
      "RMSE         576.790      29.681       565.753      116.911     \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SVR is sensitive to feature scaling, so we need StandardScaler\n",
    "pipeline_svr = Pipeline(steps=[\n",
    "    ('preprocessor', StandardScaler()),\n",
    "    ('model', SVR(kernel='rbf', C=1.0, gamma='scale'))\n",
    "])\n",
    "\n",
    "# Évaluation avec la fonction commune\n",
    "cv_results_svr = evaluate_model(pipeline_svr, \"SVR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modèle RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Random Forest Results ===\n",
      "Metric       Train Mean   Train Std    Test Mean    Test Std    \n",
      "-----------------------------------------------------------------\n",
      "R2           0.930        0.008        0.551        0.128       \n",
      "MEAN SQUARED ERROR 23046.386    3788.055     135034.233   40369.397   \n",
      "MEAN ABSOLUTE ERROR 42.413       1.055        109.535      5.722       \n",
      "RMSE         151.310      12.320       363.511      53.797      \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Random Forest doesn't require feature scaling, but let's use it for consistency\n",
    "pipeline_rf = Pipeline(steps=[\n",
    "    # ('preprocessor', StandardScaler()),\n",
    "    ('model', RandomForestRegressor(n_estimators=500, random_state=42, max_depth=20, min_samples_split=2))\n",
    "])\n",
    "\n",
    "# Évaluation avec la fonction commune\n",
    "cv_results_rf = evaluate_model(pipeline_rf, \"Random Forest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modèle GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Gradient Boosting Results ===\n",
      "Metric       Train Mean   Train Std    Test Mean    Test Std    \n",
      "-----------------------------------------------------------------\n",
      "R2           0.966        0.006        0.657        0.206       \n",
      "MEAN SQUARED ERROR 11044.831    1151.760     105598.934   63015.261   \n",
      "MEAN ABSOLUTE ERROR 60.788       2.372        106.973      7.832       \n",
      "RMSE         104.951      5.481        309.677      98.485      \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# GradientBoostingRegressor\n",
    "pipeline_gb = Pipeline(steps=[\n",
    "\t# ('preprocessor', StandardScaler()),\n",
    "\t('model', GradientBoostingRegressor(n_estimators=300, random_state=42, max_depth=3, learning_rate=0.05, subsample=1.0))\n",
    "])\n",
    "\n",
    "# Évaluation avec la fonction commune\n",
    "cv_results_gb = evaluate_model(pipeline_gb, \"Gradient Boosting\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparaison des 4 modèles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== COMPARAISON DES MODÈLES ===\n",
      "           Model  R2_Train  R2_Test  MAE_Train  MAE_Test  RMSE_Train  RMSE_Test\n",
      "LinearRegression     0.451    0.363    176.095   181.801     422.407    438.370\n",
      "             SVR    -0.022   -0.024    134.183   134.902     576.790    565.753\n",
      "    RandomForest     0.930    0.551     42.413   109.535     151.310    363.511\n",
      "GradientBoosting     0.966    0.657     60.788   106.973     104.951    309.677\n",
      "\n",
      "=== MEILLEUR MODÈLE PAR MÉTRIQUE ===\n",
      "Meilleur R² Test: GradientBoosting (R² = 0.657)\n",
      "Meilleur MAE Test: GradientBoosting (MAE = 107)\n",
      "Meilleur RMSE Test: GradientBoosting (RMSE = 310)\n",
      "\n",
      "=== ANALYSE DU SURAPPRENTISSAGE (Train R² - Test R²) ===\n",
      "LinearRegression: 0.088\n",
      "SVR: 0.002\n",
      "RandomForest: 0.379\n",
      "GradientBoosting: 0.309\n",
      "\n",
      "Modèle le moins sujet au surapprentissage: SVR\n"
     ]
    }
   ],
   "source": [
    "# Create comparison table\n",
    "import pandas as pd\n",
    "\n",
    "# Extract results from cross-validation\n",
    "models_comparison = pd.DataFrame({\n",
    "    'Model': ['LinearRegression', 'SVR', 'RandomForest', 'GradientBoosting'],\n",
    "    'R2_Train': [\n",
    "        cv_results[\"train_r2\"].mean(),\n",
    "        cv_results_svr[\"train_r2\"].mean(),\n",
    "        cv_results_rf[\"train_r2\"].mean(),\n",
    "        cv_results_gb[\"train_r2\"].mean()\n",
    "    ],\n",
    "    'R2_Test': [\n",
    "        cv_results[\"test_r2\"].mean(),\n",
    "        cv_results_svr[\"test_r2\"].mean(),\n",
    "        cv_results_rf[\"test_r2\"].mean(),\n",
    "        cv_results_gb[\"test_r2\"].mean()\n",
    "    ],\n",
    "    'MAE_Train': [\n",
    "        (-cv_results[\"train_neg_mean_absolute_error\"]).mean(),\n",
    "        (-cv_results_svr[\"train_neg_mean_absolute_error\"]).mean(),\n",
    "        (-cv_results_rf[\"train_neg_mean_absolute_error\"]).mean(),\n",
    "        (-cv_results_gb[\"train_neg_mean_absolute_error\"]).mean()\n",
    "    ],\n",
    "    'MAE_Test': [\n",
    "        (-cv_results[\"test_neg_mean_absolute_error\"]).mean(),\n",
    "        (-cv_results_svr[\"test_neg_mean_absolute_error\"]).mean(),\n",
    "        (-cv_results_rf[\"test_neg_mean_absolute_error\"]).mean(),\n",
    "        (-cv_results_gb[\"test_neg_mean_absolute_error\"]).mean()\n",
    "    ],\n",
    "    'RMSE_Train': [\n",
    "        np.sqrt(-cv_results[\"train_neg_mean_squared_error\"]).mean(),\n",
    "        np.sqrt(-cv_results_svr[\"train_neg_mean_squared_error\"]).mean(),\n",
    "        np.sqrt(-cv_results_rf[\"train_neg_mean_squared_error\"]).mean(),\n",
    "        np.sqrt(-cv_results_gb[\"train_neg_mean_squared_error\"]).mean()\n",
    "    ],\n",
    "    'RMSE_Test': [\n",
    "        np.sqrt(-cv_results[\"test_neg_mean_squared_error\"]).mean(),\n",
    "        np.sqrt(-cv_results_svr[\"test_neg_mean_squared_error\"]).mean(),\n",
    "        np.sqrt(-cv_results_rf[\"test_neg_mean_squared_error\"]).mean(),\n",
    "        np.sqrt(-cv_results_gb[\"test_neg_mean_squared_error\"]).mean()\n",
    "    ]\n",
    "})\n",
    "\n",
    "# Format the table for better readability\n",
    "models_comparison = models_comparison.round(3)\n",
    "print(\"=== COMPARAISON DES MODÈLES ===\")\n",
    "print(models_comparison.to_string(index=False))\n",
    "\n",
    "# Find best model for each metric\n",
    "print(\"\\n=== MEILLEUR MODÈLE PAR MÉTRIQUE ===\")\n",
    "print(f\"Meilleur R² Test: {models_comparison.loc[models_comparison['R2_Test'].idxmax(), 'Model']} (R² = {models_comparison['R2_Test'].max():.3f})\")\n",
    "print(f\"Meilleur MAE Test: {models_comparison.loc[models_comparison['MAE_Test'].idxmin(), 'Model']} (MAE = {models_comparison['MAE_Test'].min():.0f})\")\n",
    "print(f\"Meilleur RMSE Test: {models_comparison.loc[models_comparison['RMSE_Test'].idxmin(), 'Model']} (RMSE = {models_comparison['RMSE_Test'].min():.0f})\")\n",
    "\n",
    "# Calculate overfitting indicator (difference between train and test R²)\n",
    "models_comparison['Overfitting'] = models_comparison['R2_Train'] - models_comparison['R2_Test']\n",
    "print(f\"\\n=== ANALYSE DU SURAPPRENTISSAGE (Train R² - Test R²) ===\")\n",
    "for i, row in models_comparison.iterrows():\n",
    "    print(f\"{row['Model']}: {row['Overfitting']:.3f}\")\n",
    "    \n",
    "print(f\"\\nModèle le moins sujet au surapprentissage: {models_comparison.loc[models_comparison['Overfitting'].idxmin(), 'Model']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "OC_P3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
